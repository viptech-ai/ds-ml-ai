{"cells":[{"metadata":{},"cell_type":"markdown","source":"**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n"},{"metadata":{},"cell_type":"markdown","source":"By encoding **categorical variables**, you'll obtain your best results thus far!\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex3 import *\nprint(\"Setup Complete\")","execution_count":15,"outputs":[{"output_type":"stream","text":"Setup Complete\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"In this exercise, you will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id') \nX_test = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to print the first five rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"     MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\nId                                                                        \n619          20       RL    11694   Pave      Reg         Lvl    AllPub   \n871          20       RL     6600   Pave      Reg         Lvl    AllPub   \n93           30       RL    13360   Pave      IR1         HLS    AllPub   \n818          20       RL    13265   Pave      IR1         Lvl    AllPub   \n303          20       RL    13704   Pave      IR1         Lvl    AllPub   \n\n    LotConfig LandSlope Neighborhood  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\nId                                    ...                                       \n619    Inside       Gtl      NridgHt  ...         108             0         0   \n871    Inside       Gtl        NAmes  ...           0             0         0   \n93     Inside       Gtl      Crawfor  ...           0            44         0   \n818   CulDSac       Gtl      Mitchel  ...          59             0         0   \n303    Corner       Gtl      CollgCr  ...          81             0         0   \n\n    ScreenPorch  PoolArea  MiscVal  MoSold  YrSold SaleType SaleCondition  \nId                                                                         \n619         260         0        0       7    2007      New       Partial  \n871           0         0        0       8    2009       WD        Normal  \n93            0         0        0       8    2009       WD        Normal  \n818           0         0        0       7    2008       WD        Normal  \n303           0         0        0       1    2006       WD        Normal  \n\n[5 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>Neighborhood</th>\n      <th>...</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>619</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>11694</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>NridgHt</td>\n      <td>...</td>\n      <td>108</td>\n      <td>0</td>\n      <td>0</td>\n      <td>260</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2007</td>\n      <td>New</td>\n      <td>Partial</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>6600</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>NAmes</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>30</td>\n      <td>RL</td>\n      <td>13360</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>HLS</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Crawfor</td>\n      <td>...</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>13265</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>CulDSac</td>\n      <td>Gtl</td>\n      <td>Mitchel</td>\n      <td>...</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>13704</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>CollgCr</td>\n      <td>...</td>\n      <td>81</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Notice that the dataset contains both numerical and categorical variables.  You'll need to encode the categorical data before training a model.\n\nTo compare different models, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Drop columns with categorical data\n\nYou'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.dtypes","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"MSSubClass        int64\nMSZoning         object\nLotArea           int64\nStreet           object\nLotShape         object\nLandContour      object\nUtilities        object\nLotConfig        object\nLandSlope        object\nNeighborhood     object\nCondition1       object\nCondition2       object\nBldgType         object\nHouseStyle       object\nOverallQual       int64\nOverallCond       int64\nYearBuilt         int64\nYearRemodAdd      int64\nRoofStyle        object\nRoofMatl         object\nExterior1st      object\nExterior2nd      object\nExterQual        object\nExterCond        object\nFoundation       object\nBsmtFinSF1        int64\nBsmtFinSF2        int64\nBsmtUnfSF         int64\nTotalBsmtSF       int64\nHeating          object\nHeatingQC        object\nCentralAir       object\n1stFlrSF          int64\n2ndFlrSF          int64\nLowQualFinSF      int64\nGrLivArea         int64\nBsmtFullBath      int64\nBsmtHalfBath      int64\nFullBath          int64\nHalfBath          int64\nBedroomAbvGr      int64\nKitchenAbvGr      int64\nKitchenQual      object\nTotRmsAbvGrd      int64\nFunctional       object\nFireplaces        int64\nGarageCars        int64\nGarageArea        int64\nPavedDrive       object\nWoodDeckSF        int64\nOpenPorchSF       int64\nEnclosedPorch     int64\n3SsnPorch         int64\nScreenPorch       int64\nPoolArea          int64\nMiscVal           int64\nMoSold            int64\nYrSold            int64\nSaleType         object\nSaleCondition    object\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the lines below: drop columns in training and validation data\ndrop_X_train = X_train.loc[:,X_train.dtypes != 'object']\ndrop_X_valid = X_valid.loc[:,X_train.dtypes != 'object']\n\n# Check your answers\nstep_1.check()","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Drop\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\nstep_1.hint()\nstep_1.solution()","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"questionId\": \"1_Drop\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: Use the [`select_dtypes()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html) method to drop all columns with the `object` dtype.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> Use the [`select_dtypes()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html) method to drop all columns with the `object` dtype."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"1_Drop\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# Drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# Drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n```"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","execution_count":22,"outputs":[{"output_type":"stream","text":"MAE from Approach 1 (Drop categorical variables):\n17837.82570776256\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Label encoding\n\nBefore jumping into label encoding, we'll investigate the dataset.  Specifically, we'll look at the `'Condition2'` column.  The code cell below prints the unique entries in both the training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())","execution_count":23,"outputs":[{"output_type":"stream","text":"Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n\nUnique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"If you now write code to: \n- fit a label encoder to the training data, and then \n- use it to transform both the training and validation data, \n\nyou'll get an error.  Can you see why this is the case?  (_You'll need  to use the above output to answer this question._)"},{"metadata":{"trusted":true},"cell_type":"code","source":"step_2.a.hint()","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 4, \"questionId\": \"2.1_LabelA\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: Are there any values that appear in the validation data but not in the training data?","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> Are there any values that appear in the validation data but not in the training data?"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nstep_2.a.solution()","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2.1_LabelA\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value **that appears in the training data**. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.  Notice that the `'Condition2'` column in the validation data contains the values `'RRAn'` and `'RRNn'`, but these don't appear in the training data -- thus, if we try to use a label encoder with scikit-learn, the code will throw an error.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value **that appears in the training data**. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.  Notice that the `'Condition2'` column in the validation data contains the values `'RRAn'` and `'RRNn'`, but these don't appear in the training data -- thus, if we try to use a label encoder with scikit-learn, the code will throw an error."},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue.  For instance, you can write a custom label encoder to deal with new categories.  The simplest approach, however, is to drop the problematic categorical columns.  \n\nRun the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely label encoded are stored in `good_label_cols`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","execution_count":26,"outputs":[{"output_type":"stream","text":"Categorical columns that will be label encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n\nCategorical columns that will be dropped from the dataset: ['RoofMatl', 'Condition2', 'Functional', 'LandSlope', 'HeatingQC', 'RoofStyle', 'Condition1', 'ExterCond', 'Neighborhood', 'SaleType', 'Heating', 'Exterior2nd', 'Foundation', 'Exterior1st', 'Utilities']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to label encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `label_X_train` and `label_X_valid`, respectively.  \n- We have provided code below to drop the categorical columns in `bad_label_cols` from the dataset. \n- You should label encode the categorical columns in `good_label_cols`.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply label encoder \n# Your code here\nle = LabelEncoder()\nfor col in good_label_cols:\n    label_X_train[col] = le.fit_transform(label_X_train[col])\n    label_X_valid[col] = le.transform(label_X_valid[col])\n    \n# Check your answer\nstep_2.b.check()","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2.2_LabelB\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\nstep_2.b.hint()\nstep_2.b.solution()","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"questionId\": \"2.2_LabelB\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: Use the `LabelEncoder` class from scikit-learn. You should only encode the columns in `good_label_cols`.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> Use the `LabelEncoder` class from scikit-learn. You should only encode the columns in `good_label_cols`."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"2.2_LabelB\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n\n```"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 2 (Label Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","execution_count":29,"outputs":[{"output_type":"stream","text":"MAE from Approach 2 (Label Encoding):\n17575.291883561644\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Investigating cardinality\n\nSo far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[('Street', 2),\n ('Utilities', 2),\n ('CentralAir', 2),\n ('LandSlope', 3),\n ('PavedDrive', 3),\n ('LotShape', 4),\n ('LandContour', 4),\n ('ExterQual', 4),\n ('KitchenQual', 4),\n ('MSZoning', 5),\n ('LotConfig', 5),\n ('BldgType', 5),\n ('ExterCond', 5),\n ('HeatingQC', 5),\n ('Condition2', 6),\n ('RoofStyle', 6),\n ('Foundation', 6),\n ('Heating', 6),\n ('Functional', 6),\n ('SaleCondition', 6),\n ('RoofMatl', 7),\n ('HouseStyle', 8),\n ('Condition1', 9),\n ('SaleType', 9),\n ('Exterior1st', 15),\n ('Exterior2nd', 16),\n ('Neighborhood', 25)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"{'MSZoning': 5,\n 'Street': 2,\n 'LotShape': 4,\n 'LandContour': 4,\n 'Utilities': 2,\n 'LotConfig': 5,\n 'LandSlope': 3,\n 'Neighborhood': 25,\n 'Condition1': 9,\n 'Condition2': 6,\n 'BldgType': 5,\n 'HouseStyle': 8,\n 'RoofStyle': 6,\n 'RoofMatl': 7,\n 'Exterior1st': 15,\n 'Exterior2nd': 16,\n 'ExterQual': 4,\n 'ExterCond': 5,\n 'Foundation': 6,\n 'Heating': 6,\n 'HeatingQC': 5,\n 'CentralAir': 2,\n 'KitchenQual': 4,\n 'Functional': 6,\n 'PavedDrive': 3,\n 'SaleType': 9,\n 'SaleCondition': 6}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.items()","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"dict_items([('MSZoning', 5), ('Street', 2), ('LotShape', 4), ('LandContour', 4), ('Utilities', 2), ('LotConfig', 5), ('LandSlope', 3), ('Neighborhood', 25), ('Condition1', 9), ('Condition2', 6), ('BldgType', 5), ('HouseStyle', 8), ('RoofStyle', 6), ('RoofMatl', 7), ('Exterior1st', 15), ('Exterior2nd', 16), ('ExterQual', 4), ('ExterCond', 5), ('Foundation', 6), ('Heating', 6), ('HeatingQC', 5), ('CentralAir', 2), ('KitchenQual', 4), ('Functional', 6), ('PavedDrive', 3), ('SaleType', 9), ('SaleCondition', 6)])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n\nWe refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n\nUse the output above to answer the questions below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = sum([obj > 10 for obj in object_nunique])\n\n# Fill in the line below: How many columns are needed to one-hot encode the \n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = d['Neighborhood']\n\n# Check your answers\nstep_3.a.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\nstep_3.a.hint()\nstep_3.a.solution()","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 1, \"questionId\": \"3.1_CardinalityA\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: To one-hot encode a variable, we need one column for each unique entry.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> To one-hot encode a variable, we need one column for each unique entry."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 1, \"questionId\": \"3.1_CardinalityA\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = 3\n\n# How many columns are needed to one-hot encode the\n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = 25\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = 3\n\n# How many columns are needed to one-hot encode the\n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = 25\n\n```"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the label encoding, how many entries are added?  \n\nUse your answers to fill in the lines below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: How many entries are added to the dataset by \n# replacing the column with a one-hot encoding?\nOH_entries_added = 10000*99\n\n# Fill in the line below: How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0\n\n# Check your answers\nstep_3.b.check()","execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 1, \"questionId\": \"3.2_CardinalityB\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\nstep_3.b.hint()\nstep_3.b.solution()","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 1, \"questionId\": \"3.2_CardinalityB\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: To calculate how many entries are added to the dataset through the one-hot encoding, begin by calculating how many entries are needed to encode the categorical variable (by multiplying the number of rows by the number of columns in the one-hot encoding). Then, to obtain how many entries are **added** to the dataset, subtract the number of entries in the original column.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> To calculate how many entries are added to the dataset through the one-hot encoding, begin by calculating how many entries are needed to encode the categorical variable (by multiplying the number of rows by the number of columns in the one-hot encoding). Then, to obtain how many entries are **added** to the dataset, subtract the number of entries in the original column."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 1, \"questionId\": \"3.2_CardinalityB\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# How many entries are added to the dataset by\n# replacing the column with a one-hot encoding?\nOH_entries_added = 1e4*100 - 1e4\n\n# How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# How many entries are added to the dataset by\n# replacing the column with a one-hot encoding?\nOH_entries_added = 1e4*100 - 1e4\n\n# How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0\n\n```"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: One-hot encoding\n\nIn this step, you'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","execution_count":37,"outputs":[{"output_type":"stream","text":"Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nCategorical columns that will be dropped from the dataset: ['Neighborhood', 'Exterior2nd', 'Exterior1st']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nhelp(OneHotEncoder)","execution_count":39,"outputs":[{"output_type":"stream","text":"Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n\nclass OneHotEncoder(_BaseEncoder)\n |  OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n |  \n |  Encode categorical features as a one-hot numeric array.\n |  \n |  The input to this transformer should be an array-like of integers or\n |  strings, denoting the values taken on by categorical (discrete) features.\n |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n |  encoding scheme. This creates a binary column for each category and\n |  returns a sparse matrix or dense array (depending on the ``sparse``\n |  parameter)\n |  \n |  By default, the encoder derives the categories based on the unique values\n |  in each feature. Alternatively, you can also specify the `categories`\n |  manually.\n |  \n |  This encoding is needed for feeding categorical data to many scikit-learn\n |  estimators, notably linear models and SVMs with the standard kernels.\n |  \n |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n |  instead.\n |  \n |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n |  \n |  .. versionchanged:: 0.20\n |  \n |  Parameters\n |  ----------\n |  categories : 'auto' or a list of array-like, default='auto'\n |      Categories (unique values) per feature:\n |  \n |      - 'auto' : Determine categories automatically from the training data.\n |      - list : ``categories[i]`` holds the categories expected in the ith\n |        column. The passed categories should not mix strings and numeric\n |        values within a single feature, and should be sorted in case of\n |        numeric values.\n |  \n |      The used categories can be found in the ``categories_`` attribute.\n |  \n |      .. versionadded:: 0.20\n |  \n |  drop : {'first', 'if_binary'} or a array-like of shape (n_features,),             default=None\n |      Specifies a methodology to use to drop one of the categories per\n |      feature. This is useful in situations where perfectly collinear\n |      features cause problems, such as when feeding the resulting data\n |      into a neural network or an unregularized regression.\n |  \n |      However, dropping one category breaks the symmetry of the original\n |      representation and can therefore induce a bias in downstream models,\n |      for instance for penalized linear classification or regression models.\n |  \n |      - None : retain all features (the default).\n |      - 'first' : drop the first category in each feature. If only one\n |        category is present, the feature will be dropped entirely.\n |      - 'if_binary' : drop the first category in each feature with two\n |        categories. Features with 1 or more than 2 categories are\n |        left intact.\n |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n |        should be dropped.\n |  \n |  sparse : bool, default=True\n |      Will return sparse matrix if set True else will return an array.\n |  \n |  dtype : number type, default=np.float\n |      Desired dtype of output.\n |  \n |  handle_unknown : {'error', 'ignore'}, default='error'\n |      Whether to raise an error or ignore if an unknown categorical feature\n |      is present during transform (default is to raise). When this parameter\n |      is set to 'ignore' and an unknown category is encountered during\n |      transform, the resulting one-hot encoded columns for this feature\n |      will be all zeros. In the inverse transform, an unknown category\n |      will be denoted as None.\n |  \n |  Attributes\n |  ----------\n |  categories_ : list of arrays\n |      The categories of each feature determined during fitting\n |      (in order of the features in X and corresponding with the output\n |      of ``transform``). This includes the category specified in ``drop``\n |      (if any).\n |  \n |  drop_idx_ : array of shape (n_features,)\n |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n |        to be dropped for each feature.\n |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n |        feature isn't binary.\n |      - ``drop_idx_ = None`` if all the transformed features will be\n |        retained.\n |  \n |  See Also\n |  --------\n |  sklearn.preprocessing.OrdinalEncoder : Performs an ordinal (integer)\n |    encoding of the categorical features.\n |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n |    dictionary items (also handles string-valued features).\n |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n |    encoding of dictionary items or strings.\n |  sklearn.preprocessing.LabelBinarizer : Binarizes labels in a one-vs-all\n |    fashion.\n |  sklearn.preprocessing.MultiLabelBinarizer : Transforms between iterable of\n |    iterables and a multilabel format, e.g. a (samples x classes) binary\n |    matrix indicating the presence of a class label.\n |  \n |  Examples\n |  --------\n |  Given a dataset with two features, we let the encoder find the unique\n |  values per feature and transform the data to a binary one-hot encoding.\n |  \n |  >>> from sklearn.preprocessing import OneHotEncoder\n |  \n |  One can discard categories not seen during `fit`:\n |  \n |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n |  >>> enc.fit(X)\n |  OneHotEncoder(handle_unknown='ignore')\n |  >>> enc.categories_\n |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n |  array([[1., 0., 1., 0., 0.],\n |         [0., 1., 0., 0., 0.]])\n |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n |  array([['Male', 1],\n |         [None, 2]], dtype=object)\n |  >>> enc.get_feature_names(['gender', 'group'])\n |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n |    dtype=object)\n |  \n |  One can always drop the first column for each feature:\n |  \n |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n |  >>> drop_enc.categories_\n |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n |  array([[0., 0., 0.],\n |         [1., 1., 0.]])\n |  \n |  Or drop a column for feature only having 2 categories:\n |  \n |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n |  array([[0., 1., 0., 0.],\n |         [1., 0., 1., 0.]])\n |  \n |  Method resolution order:\n |      OneHotEncoder\n |      _BaseEncoder\n |      sklearn.base.TransformerMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y=None)\n |      Fit OneHotEncoder to X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape [n_samples, n_features]\n |          The data to determine the categories of each feature.\n |      \n |      y : None\n |          Ignored. This parameter exists only for compatibility with\n |          :class:`sklearn.pipeline.Pipeline`.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  fit_transform(self, X, y=None)\n |      Fit OneHotEncoder to X, then transform X.\n |      \n |      Equivalent to fit(X).transform(X) but more convenient.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape [n_samples, n_features]\n |          The data to encode.\n |      \n |      y : None\n |          Ignored. This parameter exists only for compatibility with\n |          :class:`sklearn.pipeline.Pipeline`.\n |      \n |      Returns\n |      -------\n |      X_out : sparse matrix if sparse=True else a 2-d array\n |          Transformed input.\n |  \n |  get_feature_names(self, input_features=None)\n |      Return feature names for output features.\n |      \n |      Parameters\n |      ----------\n |      input_features : list of str of shape (n_features,)\n |          String names for input features if available. By default,\n |          \"x0\", \"x1\", ... \"xn_features\" is used.\n |      \n |      Returns\n |      -------\n |      output_feature_names : ndarray of shape (n_output_features,)\n |          Array of feature names.\n |  \n |  inverse_transform(self, X)\n |      Convert the data back to the original representation.\n |      \n |      In case unknown categories are encountered (all zeros in the\n |      one-hot encoding), ``None`` is used to represent this category.\n |      \n |      Parameters\n |      ----------\n |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n |          The transformed data.\n |      \n |      Returns\n |      -------\n |      X_tr : array-like, shape [n_samples, n_features]\n |          Inverse transformed array.\n |  \n |  transform(self, X)\n |      Transform X using one-hot encoding.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape [n_samples, n_features]\n |          The data to encode.\n |      \n |      Returns\n |      -------\n |      X_out : sparse matrix if sparse=True else a 2-d array\n |          Transformed input.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.TransformerMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : mapping of string to any\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as pipelines). The latter have parameters of the form\n |      ``<component>__<parameter>`` so that it's possible to update each\n |      component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : object\n |          Estimator instance.\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n# Check your answer\nstep_4.check()","execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 2, \"failureMessage\": \"You still need to encode some of the categorical columns in your training data.\", \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"4_OneHot\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Incorrect: You still need to encode some of the categorical columns in your training data.","text/markdown":"<span style=\"color:#cc3333\">Incorrect:</span> You still need to encode some of the categorical columns in your training data."},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\nstep_4.hint()\nstep_4.solution()","execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"questionId\": \"4_OneHot\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: Begin by applying the one-hot encoder to the low cardinality columns in the training and validation data in `X_train[low_cardinality_cols]` and `X_valid[low_cardinality_cols]`, respectively.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> Begin by applying the one-hot encoder to the low cardinality columns in the training and validation data in `X_train[low_cardinality_cols]` and `X_valid[low_cardinality_cols]`, respectively."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"4_OneHot\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\n```"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Generate test predictions and submit your results\n\nAfter you complete Step 4, if you'd like to use what you've learned to submit your results to the leaderboard, you'll need to preprocess the test data before generating predictions.\n\n**This step is completely optional, and you do not need to submit results to the leaderboard to successfully complete the exercise.**\n\nCheck out the previous exercise if you need help with remembering how to [join the competition](https://www.kaggle.com/c/home-data-for-ml-course) or save your results to CSV.  Once you have generated a file with your results, follow the instructions below:\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# (Optional) Your code here","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keep going\n\nWith missing value handling and categorical encoding, your modeling process is getting complex. This complexity gets worse when you want to save your model to use in the future. The key to managing this complexity is something called **pipelines**. \n\n**[Learn to use pipelines](https://www.kaggle.com/alexisbcook/pipelines)** to preprocess datasets with categorical variables, missing values and any other messiness your data throws at you."},{"metadata":{},"cell_type":"markdown","source":"---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161289) to chat with other Learners.*"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}